{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCj3IlEz0hyy7kTiQSyA5R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kashafali8/stable-baselines3/blob/master/Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VT59suEIvGLW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install command git+https://github.com/kashafali8/stable-baselines3 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Library Imports\n",
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "7YJgCqvGxiBN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training environment of Cartpole\n",
        "train_env = gym.make('CartPole-v1')\n",
        "\n",
        "## Evalutation environment of Cartpole\n",
        "evaluation_env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "blABQ1A4xoyQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Vanilla Model\n",
        "model_vanilla = A2C('MlpPolicy', train_env, verbose=0, tensorboard_log=\"./a2c_cartpole_tensorboard/\", gae_lambda=0)\n",
        "\n",
        "## GAE Model\n",
        "model_gae = A2C('MlpPolicy', train_env, verbose=0, tensorboard_log=\"./a2c_cartpole_tensorboard/\", gae_lambda=0.75)\n",
        "\n",
        "## N-step\n",
        "model_n_step = A2C('MlpPolicy', train_env, verbose=0, tensorboard_log=\"./a2c_cartpole_tensorboard/\", gae_lambda=1, n_steps = 7)\n"
      ],
      "metadata": {
        "id": "LinJ51pDxsCN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train vanilla model\n",
        "model_vanilla.learn(total_timesteps=100000)\n",
        "\n",
        "# evaluate vanilla model\n",
        "reward_mean_vanilla, reward_std_vanilla = evaluate_policy(model_vanilla, evaluation_env, n_eval_episodes=100)\n",
        "print(f'Vanilla Model Mean Reward: {np.round(reward_mean_vanilla,2)} +/- {np.round(reward_std_vanilla,2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJFrpgAyqv92",
        "outputId": "0008b36d-cbba-432f-a4d4-c46277370a41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla Model Mean Reward: 30.18 +/- 4.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train GAE Model\n",
        "model_gae.learn(total_timesteps=100000)\n",
        "\n",
        "# evaluate GAE Model\n",
        "reward_mean_gae, reward_std_gae = evaluate_policy(model_gae, evaluation_env, n_eval_episodes=100)\n",
        "print(f'GAE Model Mean Reward: {np.round(reward_mean_gae,2)} +/- {np.round(reward_std_gae,2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o9mfzLbrhjs",
        "outputId": "89ac68e7-43f2-45e3-bc45-b2f1a2559dda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAE Model Mean Reward: 158.28 +/- 8.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train N-step model\n",
        "model_vanilla.learn(total_timesteps=100000)\n",
        "\n",
        "# evaluate N-step model\n",
        "reward_mean_nstep, reward_std_nstep = evaluate_policy(model_n_step, evaluation_env, n_eval_episodes=100)\n",
        "print(f'N-step Model Mean Reward: {np.round(reward_mean_nstep,2)} +/- {np.round(reward_std_nstep,2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJgaJoUGrlZM",
        "outputId": "ecc72505-d188-40b1-ac16-003c93b185b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-step Model Mean Reward: 77.04 +/- 46.32\n"
          ]
        }
      ]
    }
  ]
}